<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>anhcodes</title>
    <link>https://anhcodes.dev/</link>
    <description>Recent content on anhcodes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Jan 2024 17:01:45 +0000</lastBuildDate><atom:link href="https://anhcodes.dev/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Simple way to write Spark Dataframe from Azure Databricks to Azure Synapse</title>
      <link>https://anhcodes.dev/blog/databricks-to-synapse/</link>
      <pubDate>Tue, 02 Jan 2024 17:01:45 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/databricks-to-synapse/</guid>
      <description>Reprequisites To follow along with this tutorial, you will need:
Azure subscription Synapse workspace Dedicated SQL Pool Azure Databricks workspace in the same subscription Step 1: Generate Spark dataframe in Databricks notebook Below spark code generates a dataframe of 3 columns and 100M rows (around 2GB of data)
import random from pyspark.sql.functions import rand, randn, col, lit from pyspark.sql.types import StructType, StructField, TimestampType, StringType, DecimalType, IntegerType schema = StructType([ StructField(&amp;#34;purchase_ts&amp;#34;, TimestampType(), True), StructField(&amp;#34;customer_id&amp;#34;, IntegerType(), True), StructField(&amp;#34;purchase_amount&amp;#34;, DecimalType(18,2), True) ]) df = spark.</description>
    </item>
    
    <item>
      <title>Data Objects in Databricks</title>
      <link>https://anhcodes.dev/blog/data-objects/</link>
      <pubDate>Sat, 30 Dec 2023 16:02:25 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/data-objects/</guid>
      <description>Metastore and Catalog In Databricks, metadata of data Objects (tables, views, etc.) is registered in Metastore.
Previously, Databricks uses Hive metastore by default to register schemas, tables, and views. However, it&amp;rsquo;s highly recommended to upgrade to Unity Catalog to access centralized access control, auditing, lineage, and data discovery capabilities.
Catalog is a group of databases. If you want to use Unity Catalog, create and specify Catalog is required (Set up and manage Unity Catalog).</description>
    </item>
    
    <item>
      <title>Deep Dive Into Delta Lake</title>
      <link>https://anhcodes.dev/blog/deep-dive-delta-lake/</link>
      <pubDate>Mon, 07 Aug 2023 11:23:00 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/deep-dive-delta-lake/</guid>
      <description>Delta LakeÂ is anÂ open source storage layerÂ that brings reliability toÂ data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing. Delta Lake runs on top of your existing data lake and is fully compatible with Apache Spark APIs. Delta Lake uses versioned Parquet files to store your data in your cloud storage. Apart from the versions, Delta Lake also stores a transaction log to keep track of all the commits made to the table or blob store directory to provide ACID transactions.</description>
    </item>
    
    <item>
      <title>Spark working internals, and why should you care?</title>
      <link>https://anhcodes.dev/blog/tune-spark/</link>
      <pubDate>Sat, 27 May 2023 21:48:00 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/tune-spark/</guid>
      <description>Most Big Data developers and Data Engineers start learning Spark by writing SparkSQL codes to perform ETL on DataFrame (I know I did). I also wrote a post about SparkSQL Programming. However, we quickly learn that thereâ€™s more knowlege required to go from processing a few GBs of data to dealing with TBs and PBs of data, which is a challenge for big enterprises. Learning to write correct Spark codes is only a small part of the battle, you will need to understand the Spark Architecture and Spark working internals to correct tune Spark to handle true big data, and itâ€™s the focus of this post.</description>
    </item>
    
    <item>
      <title>Spark SQL Programming Primer</title>
      <link>https://anhcodes.dev/blog/spark-sql-programming/</link>
      <pubDate>Tue, 23 May 2023 17:09:34 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/spark-sql-programming/</guid>
      <description>TL,DR - SparkSQL is a huge component of Spark Programming. This post introduces programming in SparkSQL through Spark DataFrame API. It&amp;rsquo;s important to be aware of Spark SQL built-in functions to be a more efficient Spark programmer
What is SparkSQL SparkSQL is one of the 4 APIs in Spark ecosystems. SparkSQL provides structured data processing with interfaces such as SQL or Dataframe API using Python, Scala, R, Java programming languages</description>
    </item>
    
    <item>
      <title>Debug long running Spark job</title>
      <link>https://anhcodes.dev/blog/debug-spark/</link>
      <pubDate>Mon, 22 May 2023 17:09:41 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/debug-spark/</guid>
      <description>You Spark job is running for a long time, what to do?
Generally, long-running Spark jobs can be due to various factors. We like to call them the 5S - Spill, Skew, Shuffle, Storage, and Serialization. So, how do we identify the main culprit?
ðŸ”Ž Look for Skew: Are some of the tasks taking longer than others? Do you have a join operation?
ðŸ”Ž Look for Spill: Any out-of-memory errors? Do the executors have enough memory to finish their tasks?</description>
    </item>
    
    <item>
      <title>Navigate Databricks FileSystem</title>
      <link>https://anhcodes.dev/blog/dbfs/</link>
      <pubDate>Tue, 11 Apr 2023 22:13:33 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/dbfs/</guid>
      <description> View Fullscreen View on Github </description>
    </item>
    
    <item>
      <title>Deploy Debezium and Kafka on AKS using Strimzi Operator</title>
      <link>https://anhcodes.dev/blog/deploy-debezium-aks/</link>
      <pubDate>Sat, 07 Jan 2023 00:19:27 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/deploy-debezium-aks/</guid>
      <description>This tutorial follows Debezium official documentation Deploying Debezium on Kubernetes, but modified for Azure Kubernetes Service and Azure Container Registry.
Table of Content 1. Prerequisites You need an Azure account, Azure subscription, and Azure resource group before getting started. You also need to have some understanding of CLI, Docker, Kafka, Debezium, and Kubernetes
Azure CLI If you haven&amp;rsquo;t set up your Azure CLI environment, follow this documentation to set it up.</description>
    </item>
    
    <item>
      <title>How to recover lost files after a git reset --hard</title>
      <link>https://anhcodes.dev/blog/recover-lost-files-git-reset-hard/</link>
      <pubDate>Tue, 27 Dec 2022 11:30:20 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/recover-lost-files-git-reset-hard/</guid>
      <description>While working on a recent project, I accidentally committed some files. Instead of using git reset --soft &amp;lt;prev-commit-id&amp;gt; to unstage them, I used git reset --hard HEAD and all of my new changes gone with the wind. After panicking for a few minutes, I determined to learn how git reset works and how I can revert the damages.
1. What is git reset (hard vs soft) git reset --hard resets the current branch tip, and also deletes any changes in the working directory and staging area (although files under git stash will not be affected).</description>
    </item>
    
    <item>
      <title>Query Performance Tuning in Dedicated SQL Pool (Azure Synapse Analytics)</title>
      <link>https://anhcodes.dev/blog/query-performance/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/query-performance/</guid>
      <description>1. Best practices when Creating Tables When creating table in Dedicated SQL Pool, choose the correct Distribution Column and Index for best query performance. Follow Best Practices in creating tables in Azure Synapse Analytics
Remember to create and Update Stats on your new tables Table statistics for dedicated SQL pool in Azure Synapse Analytics
2. Identify slow-running QID and analyze compilation To best determine what is causing any given query&amp;rsquo;s slow performance, you will need to identify an example long-running query:</description>
    </item>
    
    <item>
      <title>Flower Classifcation with Pytorch</title>
      <link>https://anhcodes.dev/portfolio/flower-classifier/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://anhcodes.dev/portfolio/flower-classifier/</guid>
      <description> View Fullscreen View on Github </description>
    </item>
    
    <item>
      <title>Python Stock Price Prediction</title>
      <link>https://anhcodes.dev/portfolio/stock-prediction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://anhcodes.dev/portfolio/stock-prediction/</guid>
      <description> View Fullscreen View on Github </description>
    </item>
    
  </channel>
</rss>
