<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Debug long running Spark job</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />

  <!-- Slick Carousel -->
  <link rel="stylesheet" href="https://anhcodes.dev/plugins/slick/slick.css" />
  <link rel="stylesheet" href="https://anhcodes.dev/plugins/slick/slick-theme.css" />
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://anhcodes.dev/plugins/font-awesome/css/font-awesome.min.css" />

  <!-- Magnific Popup -->
  <link rel="stylesheet" href="https://anhcodes.dev/plugins/magnafic-popup/magnific-popup.css" />

  <!-- Stylesheets -->
  
  <link href="https://anhcodes.dev/scss/style.min.css" rel="stylesheet" />

  <!--Favicon-->
  <link rel="shortcut icon" href="https://anhcodes.dev/images/favicon.ico" type="image/x-icon" />
  <link rel="icon" href="https://anhcodes.dev/images/favicon.png" type="image/x-icon" />
  
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-9145PR72NN"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-9145PR72NN');
  </script>
  <script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4.37.1"></script>
  <script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.11.0/dist/algoliasearch.umd.min.js"></script>
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css" integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin="anonymous">  
  
</head>

<body>
  <nav class="navbar navbar-expand-lg fixed-top">
  <div class="container">
    <a href="https://anhcodes.dev/" class="navbar-brand">
      <img src="https://anhcodes.dev/images/favicon.png" alt="site-logo">
    </a>
    <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#navbarCollapse">
      <span class="navbar-toggler-icon"></span>
      <span class="navbar-toggler-icon"></span>
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse justify-content-between" id="navbarCollapse">
      <ul class="nav navbar-nav main-navigation my-0 mx-auto">
        
        
        <li class="nav-item">
          <a href="https://anhcodes.dev/#home"
            class="nav-link text-dark text-sm-center p-2 ">Home</a>
        </li>
        
        <li class="nav-item">
          <a href="https://anhcodes.dev/#about"
            class="nav-link text-dark text-sm-center p-2 ">About</a>
        </li>
        
        <li class="nav-item">
          <a href="https://anhcodes.dev/#portfolio"
            class="nav-link text-dark text-sm-center p-2 ">Portfolio</a>
        </li>
        
        <li class="nav-item">
          <a href="https://anhcodes.dev/#resume"
            class="nav-link text-dark text-sm-center p-2 ">Experiences</a>
        </li>
        
        <li class="nav-item">
          <a href="https://anhcodes.dev/#skills"
            class="nav-link text-dark text-sm-center p-2 ">Skills</a>
        </li>
        
        <li class="nav-item">
          <a href="https://anhcodes.dev/#testimonial"
            class="nav-link text-dark text-sm-center p-2 ">Testimonials</a>
        </li>
        
        <li class="nav-item">
          <a href="https://anhcodes.dev/#blog"
            class="nav-link text-dark text-sm-center p-2 ">Blog</a>
        </li>
        
      </ul>
      <div class="navbar-nav">
        <a href="https://anhcodes.dev/contact" class="btn btn-primary btn-zoom hire_button">Contact Me</a>
      </div>
    </div>
  </div>
</nav>
  <div id="content">
    

<header class="breadCrumb">
  <div class="container">
    <div class="row">
      <div class="col-lg-10 col-md-12 offset-lg-1 offset-md-0 text-center">
        <h3 class="breadCrumb__title">Debug long running Spark job</h3>
        <nav aria-label="breadcrumb" class="d-flex justify-content-center">
          <ol class="breadcrumb align-items-center">
            <li class="breadcrumb-item"><a href=https://anhcodes.dev/>Home</a></li>
            <li class="breadcrumb-item"><a href=https://anhcodes.dev/blog>All Posts</a></li>
            <li class="breadcrumb-item"><a href=https://anhcodes.dev//tags>All Tags</a></li>
          </ol>
        </nav>
      </div>
    </div>

    <ul class="post-meta">
      <li>
        <i class="fa fa-calendar"></i>
        May 22, 2023
      </li>
      <li>
        <i class="fa fa-clock-o"></i>
        12 mins read
      </li>
      <li>
        <i class="fa fa-user"></i>
        Anh Chu
      </li>
    </ul>
    <ul class="post-meta">
      </li>
      
      <li>
        <i class="fa fa-tag"></i>
        
          <a class="text-lowercase" href="https://anhcodes.dev/tags/spark/">spark</a>
        
      </li>
      
    </ul>
  </div>
  </div>
  </div>
  </div>
</header>

<section class="section singleBlog">
  <div class="svg-img">
    <img src=https://anhcodes.dev/images/hero/hero-background-svg.svg alt="">
  </div>
  <div class="animate-shape">
    <img src=https://anhcodes.dev/images/skill/skill-background-shape.svg alt="">
  </div>
  <div class="animate-pattern">
    <img src=https://anhcodes.dev/images/service/background-pattern.svg alt="background-shape">
  </div>
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <div class="singleBlog__feature">
          <img src=https://anhcodes.dev/images/inpost/debug-spark/0.png alt="feature-image">
          <center><figcaption style="font-size: 10px; margin-top: 20px;">  </figcaption></center>
        </div>
      </div>
    </div>
    <div class="row mt-5">
      <div class="col-lg-12">
        <div class="singleBlog__content">

          

          <p>You Spark job is running for a long time, what to do? Generally, long-running Spark jobs can be due to various factors. We like to call them the 5S - Spill, Skew, Shuffle, Storage, and Serialization. So, how do we identify the main culprit?</p>
<p>ðŸ”Ž Look for Skew: Are some of the tasks taking longer than others? Do you have a join operation?</p>
<p>ðŸ”Ž Look for Spill: Any out-of-memory errors? Do the executors have enough memory to finish their tasks? Got any disk spills?</p>
<p>ðŸ”Ž Look for Shuffle: Large amounts of data being shuffled across executors? Do you have a join operation?</p>
<p>ðŸ”Ž Look for Storage Issues: Do you have small files or highly nested directory structure?</p>
<p>ðŸ”Ž Look for inefficient Spark code, or serialization issues.</p>
<p>Now, the solution may vary depending on the root cause. Most of the times, the root cause indicators will show up in the SparkUI and itâ€™s important that you understand how to read it.</p>
<p>In general, always try to cut down the time Spark takes to load data into memory, parallelize tasks across executors, and scale the memory according to data size.</p>
<div class="toc">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#skew-imbalance-partitions">Skew [imbalance partitions]</a>
      <ul>
        <li><a href="#how-do-you-identify-skew-in-your-sparkui">How do you identify Skew in your SparkUI?</a></li>
        <li><a href="#how-do-you-fix-skew">How do you fix Skew?</a></li>
      </ul>
    </li>
    <li><a href="#spill-lack-of-memory">Spill [lack of memory]</a>
      <ul>
        <li><a href="#how-do-you-identify-spill-in-your-sparkui">How do you identify Spill in your SparkUI?</a></li>
        <li><a href="#how-do-you-fix-spill">How do you fix Spill?</a></li>
      </ul>
    </li>
    <li><a href="#shuffles-data-transfer">Shuffles [data transfer]</a>
      <ul>
        <li><a href="#how-to-identify-shuffle">How to identify Shuffle?</a></li>
        <li><a href="#how-to-mitigate-shuffle">How to mitigate Shuffle?</a></li>
      </ul>
    </li>
    <li><a href="#storage-small-files-scanning-inferring-schemaschema-evolution">Storage [small files, scanning, inferring schema/schema evolution]</a>
      <ul>
        <li><a href="#how-to-identify-storage-issue">How to Identify storage issue?</a></li>
        <li><a href="#how-to-fix-storage-issue">How to fix storage issue?</a></li>
      </ul>
    </li>
    <li><a href="#serialization-api-programming">Serialization [API, programming]</a></li>
  </ul>
</nav>
</div>
  
<h2 id="skew-imbalance-partitions">Skew [imbalance partitions]</h2>
<p><em>TL,DR - Skew is caused by imbalance partitions. To fix Skew, try to repartition data, or set SkewHint when reading in data from disk to memory. If Skew is caused by partition imbalance after shuffling stage, enable SkewJoin option with Adaptive Query Execution, and set the correct shuffle partitions size. In most cases, turning on Adaptive Query Execution can help mitigate Skew issue.</em></p>
<p>Let&rsquo;s talk about one of the most common issues you might encounter in Spark - &ldquo;skew&rdquo;. Skew refers to an imbalance in partition sizes, which can also lead to a spill. When you read in data in Spark, it&rsquo;s typically divided into partitions of 128 MB, distributed evenly according to theÂ <code>maxPartitionBytes</code>Â setting. However, during transformations, Spark will need to shuffle your data, and some partitions may end up having significantly more data than others, creating skew in your data.</p>
<p>When a partition is bigger than the others, the executor will take longer to process that partition and need more memory, which might eventually result in a spill to disk or Out Of Memory (OOM) errors.</p>
<h3 id="how-do-you-identify-skew-in-your-sparkui">How do you identify Skew in your SparkUI?</h3>
<ul>
<li>If you see long running tasks and/or uneven distribution of tasks in the Event Timeline of a long-running stage</li>
<li>If you see uneven Shuffle Read/Write Size in a stageâ€™s Summary Metrics</li>
<li>Skew can cause Spill, so sometimes you will see Disk or Memory Spill as well. If Spill is caused by Skew, you have to fix Skew as the Root cause.</li>
</ul>
<div class="image">
    <style>
      .img-responsive {
        display: block;
        height: auto;
        max-width: 100%;
      }
    </style>
    <p>
      <center><img src="https://anhcodes.dev/images/inpost/debug-spark/1.png"  alt="blog-img" class="img-responsive"></center>
    </p>
  </div>
  
<div class="image">
    <style>
      .img-responsive {
        display: block;
        height: auto;
        max-width: 100%;
      }
    </style>
    <p>
      <center><img src="https://anhcodes.dev/images/inpost/debug-spark/2.png"  alt="blog-img" class="img-responsive"></center>
    </p>
  </div>
  
<h3 id="how-do-you-fix-skew">How do you fix Skew?</h3>
<p>If you disk spill or OOM errors are caused by skew, instead of solving for RAM problem, solve for uneven distribution of records across all partitions</p>
<p>Option 1: If you run Spark on Databricks, use skew hint (refer <a href="https://docs.databricks.com/optimizations/skew-join.html">Skew Join optimization</a>). For example, assuming that you know the column used in the join is skewed, set the skew hint for that column. With this skew hint information, Databricks Runtime can hopefully construct a better query plan for the join</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#433e56;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#b0bec5">## Set skew hint when loading the table</span>
</span></span><span style="display:flex;"><span>df <span style="color:#ffb8d1">=</span> spark<span style="color:#ffb8d1">.</span>read<span style="color:#ffb8d1">.</span>format(<span style="color:#1bc5e0">&#34;delta&#34;</span>)<span style="color:#ffb8d1">.</span>load(trxPath)
</span></span><span style="display:flex;"><span>						<span style="color:#ffb8d1">.</span>hint(<span style="color:#1bc5e0">&#34;skew&#34;</span>,<span style="color:#1bc5e0">&#34;&lt;join_column&gt;&#34;</span>) 
</span></span></code></pre></div><p>Option 2: If you use Spark 3.x, utilize Adaptive Query Execution (AQE). AQE can automatically adapt query plans at runtime based on more accurate metrics, leading to more optimized execution. After a shuffle, AQE can automatically use split shuffle read to split skewed partitions into smaller partitions, ensuring that the executors are not burdened by larger skews. This can lead to faster query execution and a more efficient use of cluster resources. <strong>Using AQE is highly recommended, and generally more effective than other options</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#433e56;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#b0bec5">## Turn AQE on</span>
</span></span><span style="display:flex;"><span>spark<span style="color:#ffb8d1">.</span>conf<span style="color:#ffb8d1">.</span>set(<span style="color:#1bc5e0">&#39;spark.sql.adaptive.enabled&#39;</span>, <span style="color:#c2ffdf">True</span>)
</span></span><span style="display:flex;"><span>spark<span style="color:#ffb8d1">.</span>conf<span style="color:#ffb8d1">.</span>set(<span style="color:#1bc5e0">&#39;spark.sql.adaptive.skewedJoin.enabled&#39;</span>, <span style="color:#c2ffdf">True</span>)
</span></span></code></pre></div><p>Option3 : Another option is to salt the skewed column with a random number to create more evenly partitions but at the cost of extra processing. This is a more complex operations, which we should discuss in another post.</p>
<h2 id="spill-lack-of-memory">Spill [lack of memory]</h2>
<p><em>TL,DR - Spill is caused by executors lacking of memory to process partitions. To fix Spill think about how you can add more memory to the executors, or manage the partitions sizes</em></p>
<p>If your Spark executors donâ€™t have enough local memory to process their allocated partitions, Spark has to spill the data to disk. Spill is a Sparkâ€™s measure of moving an RDD from local RAM to disk and then back to executorâ€™s RAM again for further processing with the goal of avoiding an out-of-memory error. However, this can lead to expensive disk reads and writes, and significantly slow down the entire job.</p>
<p>This process occurs when a partition becomes too large to fit into RAM, and it may be a result of skew in the data. Some potential causes of spill include settingÂ <code>spark.sql.files.maxPartitionBytes</code>Â too high, usingÂ <code>explode()</code>Â on an array, performing aÂ <code>join</code>Â orÂ <code>crossJoin</code>Â of two tables, or aggregating results by a skewed column.</p>
<h3 id="how-do-you-identify-spill-in-your-sparkui">How do you identify Spill in your SparkUI?</h3>
<p>You can  find Spill indicators on SparkUI under each stageâ€™s detail tabe or Aggregated Metrics by Executor. When data is spilled, both the size of the data in memory and on disk will be provided. Typically, the size on disk will be smaller due to compression that occurs when serializing data before it is written to disk.</p>
<div class="image">
    <style>
      .img-responsive {
        display: block;
        height: auto;
        max-width: 100%;
      }
    </style>
    <p>
      <center><img src="https://anhcodes.dev/images/inpost/debug-spark/3.png"  alt="blog-img" class="img-responsive"></center>
    </p>
  </div>
  
<div class="image">
    <style>
      .img-responsive {
        display: block;
        height: auto;
        max-width: 100%;
      }
    </style>
    <p>
      <center><img src="https://anhcodes.dev/images/inpost/debug-spark/4.png"  alt="blog-img" class="img-responsive"></center>
    </p>
  </div>
  
<h3 id="how-do-you-fix-spill">How do you fix Spill?</h3>
<p>To mitigate Spill issues, there are several actions you can take.</p>
<ul>
<li>First, make sure to fix the root cause of skew if this is the underlying issue behind the spill. To decrease partition sizes, increase the number of partitions or use explicit repartitioning.</li>
<li>If above doesnâ€™t work, allocate a cluster with more memory per worker if each worker need to process bigger partitions of data.</li>
<li>Finally, you can adjust specific configuration settings usingÂ <code>spark.conf.set()</code>Â to manage the size and number of partitions.
<ul>
<li>manage <code>spark.conf.set(spark.sql.shuffle.partitions, {num_partitions})</code> to reduce data in each partition</li>
<li>manage <code>spark.conf.set(â€™spark.sql.files.maxPartitionBytesâ€™, {MB}*1024*1024)</code> to reduce size of partition when Spark read from disk to memory</li>
</ul>
</li>
</ul>
<p>It&rsquo;s worth noting that ignoring spill may not always be a good idea, as even a small percentage of tasks can delay an entire process. Therefore, it&rsquo;s important to take note of spills and manage them proactively to enhance the performance of your Spark jobs.</p>
<h2 id="shuffles-data-transfer">Shuffles [data transfer]</h2>
<p><em>TL,DR - Shuffle refers to the movement of data across executors, and it&rsquo;s inevitable with wide transformation jobs. To tune shuffle operations, think about how you can reduce the amount of data that get shuffled across the cluster network</em></p>
<div class="image">
    <style>
      .img-responsive {
        display: block;
        height: auto;
        max-width: 100%;
      }
    </style>
    <p>
      <center><img src="https://anhcodes.dev/images/inpost/debug-spark/5.png"  alt="blog-img" class="img-responsive"></center>
    </p>
  </div>
  
<p>Shuffle is the act of moving data between executors. If you have multiple data partitions in different executors on a Spark cluster, shuffle is necessary and inevitable. Most of the time, shuffle operations are actually quite fast. However, there are situations when shuffle can become the culprit of slowing down your Spark job. For example, moving data across the network is slow, and the more data you have to move, the slower it will get.  Moreover, Incorrect shuffles can cause Skew, and potentital Spill</p>
<h3 id="how-to-identify-shuffle">How to identify Shuffle?</h3>
<p>If you use wide transformation (<code>distinct</code>, <code>join</code>, <code>groupBy/count</code>, <code>orderBy</code>, <code>crossJoin</code>) in your Spark job and you have multiple executors in the Spark cluster, shuffle will most likely happen.</p>
<p>In the SparkUI, you can find the Shuffle Read Size and Shuffle Write Size numbers in the stages on the SparkUI</p>
<h3 id="how-to-mitigate-shuffle">How to mitigate Shuffle?</h3>
<p>To reduce the impact of shuffle on your Spark job, try to reduce the amount of data you have to shuffle across network</p>
<h4 id="1-tune-the-spark-cluster">1. Tune the Spark Cluster</h4>
<ul>
<li>Use fewer and larger workers: You normally pay the same unit price for the same total number of cores and memory in your cluster, no matter the number of executors. So if you have jobs that require a lot of wide transformations, choose the bigger instance and less workers. That way you don&rsquo;t have many executors to exchange data.</li>
</ul>
<h4 id="2-limit-shuffled-data">2. Limit shuffled data</h4>
<ul>
<li>Use predicate push down and/or narrow the columns to reduce the amount of data being shuffled</li>
</ul>
<h4 id="3-try-broadcasthashjoin-if-possible">3. Try BroadcastHashJoin if possible</h4>
<ul>
<li>Use BroadcastHashJoin if 1 table is less than 10MB. With BHJ, the smaller table will be broadcasted to all executors, which can eliminate the shuffle of data.
<ul>
<li>Step 1: each executor will read in their assigned partitions from the bigger table</li>
<li>Step 2: every partition of the broadcasted table is sent to the driver (therefore you want to make sure the broadcasted table is small enough to fit into the driver mem)</li>
<li>Step 3: a copy of the entire broadcasted is sent to each executor</li>
<li>Step 4: each executor can do a standard join between tables because they have the full copy of the broadcasted table, therefore shuffle can be avoided in join</li>
<li>There are few considerations when it comes to BroadcastHashJoin you should be aware of that we can discuss in another post</li>
</ul>
</li>
<li>With Adaptive Query Execution in Spark 3.x, Spark will try to do BHJ in runtime if one of the table is small enough</li>
</ul>
<h4 id="4-bucketing">4. Bucketing</h4>
<p>Bucketing can be a useful technique to improve the efficiency of data processing by pre-sorting and aggregating data in the Join operation. The process involves dividing the data into N buckets, with both tables requiring the same number of buckets. This technique can be particularly effective for large datasets of several TBs, where data is queried and joined repeatedly.</p>
<p>Bucketing should be performed by a skilled data engineer as part of the data preparation process. It&rsquo;s worth noting that bucketing is only worthwhile if the data is frequently queried and joined, and filtering does not improve the join operation.</p>
<p>By using bucketing in the Join operation, the process of shuffling and exchanging data can be eliminated, resulting in a more efficient join.</p>
<h2 id="storage-small-files-scanning-inferring-schemaschema-evolution">Storage [small files, scanning, inferring schema/schema evolution]</h2>
<p><em>TL,DR - Storage issues are problems related to how data is stored on disk, which can lead to high overhead with ingesting data by open, read, close files operation. To fix issues relating to Storage, think about how you can reduce the read, write, ingest files on disk</em></p>
<h3 id="how-to-identify-storage-issue">How to Identify storage issue?</h3>
<p>There are a few storage-related potential issues that can slow down operations in Spark data processing. One is the overhead of opening, reading, and closing many <strong>small files</strong>. To address this, it&rsquo;s recommended to aim for files with a size of 1GB or larger, which can significantly reduce the time spent on these operations.</p>
<p>Another is <strong>directory scanning issue</strong> which can arise with <strong>highly partitioned</strong> datasets that have multiple directories for each partition, requiring the driver to scan all of them on disk. For example, files on your storage are fine-grained partitioned to year-month-date-minute.</p>
<p>A third issue involves <strong>schema operations</strong>, such as inferring the schema for JSON and CSV files. This can be very costly, requiring a full read of the files to determine data types, even if you only need a subset of the data. By contrast, reading Parquet files typically only requires one-time reading of the schema, which is much faster. Therefore, it&rsquo;s recommended to use Parquet as file storage for Spark considering that Parquet stores schema information in the file</p>
<p>However, schema evolution support, even with Parquet files, can also be expensive, as it requires reading all the files and merging the schema collectively. For this reason, starting from Spark 1.5, schema merging is turned off by default and needs to be turned on by setting theÂ <code>spark.sql.parquet.mergeSchema</code>Â option.</p>
<h3 id="how-to-fix-storage-issue">How to fix storage issue?</h3>
<p>To address the issue of tiny files in a storage location, if you are using Delta Lake, consider using <code>autoOptimize</code> withÂ <code>optimizeWrite</code>Â andÂ <code>autoCompact</code>Â features. These will automatically coalesce small files into larger ones, however they have some subtle differences:</p>
<ul>
<li>Auto compaction occurs after a write to a table has succeeded and runs synchronously on the cluster that has performed the write. Auto compaction only compacts files that havenâ€™t been compacted previously.</li>
<li>Optimized writes improve file size as data is written and benefit subsequent reads on the table. Optimized writes are most effective for partitioned tables, as they reduce the number of small files written to each partition.</li>
</ul>
<p>To address the issue of slow performance when scanning directories, consider registering your data as tables in order to leverage the metastore to track the files in the storage location. This may have some initial overhead, but will benefit you in the long run by avoiding repeated directory scans.</p>
<p>To address issues with schema operations, one option is to specify the schema when reading in non-Parquet file types, though this can be a time-consuming process. Alternatively, you can register tables so that the metastore can track the table schema. The best option is to use Delta Lake which offer zero reads of schema with a metastore, and at most one reads for schema evolution. <a href="https://delta.io/">Delta Lake</a> also provides other benefits such as ACID transactions, time travel, DML operations, schema evolution, etc.</p>
<h2 id="serialization-api-programming">Serialization [API, programming]</h2>
<p><em>TL,DR - Bad codes can also slow your Spark down, always try to use spark sql built-in functions and avoid UDFs when develop your Spark codes. If UDFs are needed, try vectorized UDFs for Python and Typed Transformations for Scala</em></p>
<p>Slower Spark Jobs can sometimes occur as a result of suboptimal code. One example of this would be code segments that have not been reworked to support more efficient Spark operations.</p>
<p>As a rule of thumb, always useÂ <code>spark.sql.functions</code>Â whenever possible, regardless of which language you&rsquo;re using. You can expect similar performance for both Python or Scala with these functions</p>
<p>In Scala, if you have to use User-defined functions that are not supported by standard spark functions, it&rsquo;s more efficient to use Typed transformations instead of standard Scala UDFs. In general, Scala is more efficient than Python with UDFs and Typed transformation.</p>
<p>If you&rsquo;re working with Python, avoid using Spark UDFs and vectorized UDFs if possible. But if you have to use UDF, always use <a href="https://www.databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html">vectorized UDF</a></p>

        </div>
      </div>
    </div>
  </div>
</section>


  </div>
  <section class="footer" id="contact">
	<div class="footer__background_shape">
		<svg viewBox="0 0 1920 79">
			<path d="M0 0h1920v79L0 0z" data-name="Path 1450" />
		</svg>
	</div>
	<div class="container">
		<div class="row">
			<div class="col-lg-12">
				<div class="footer__cta">
					<div class="shape-1">
						<svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
							<path data-name="Path 1449"
								d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z"
								transform="translate(217.489 188.626)" />
						</svg>
					</div>
					<div class="shape-2">
						<svg xmlns="http://www.w3.org/2000/svg" width="357" height="315.029" viewBox="0 0 357 315.029">
							<path data-name="Path 1449"
								d="M76.1-157.222C91.746-135.8 87.2-94.273 99.993-61.945c12.7 32.328 42.661 55.459 39.248 73.282-3.318 17.823-40.007 30.337-65.6 43.325-25.5 12.988-39.912 26.545-60.01 42.566-20.1 16.116-46.074 34.6-63.328 27.682-17.349-6.921-25.976-39.153-59.915-59.82s-93.1-29.768-105.325-51.478 22.373-56.028 43.609-93.949c21.331-37.921 29.2-79.35 53.563-96.793 24.459-17.444 65.414-10.9 103.9-6.921 38.396 3.982 74.326 5.404 89.965 26.829z"
								transform="translate(217.489 188.626)" />
						</svg>
					</div>
					<div class="text-light footer__cta_content">
						<span>Contact me</span>
						<h2 class="mb-0">Letâ€™s Work Together</h2>
					</div>
					<div class="footer__cta_action">
						<a class="btn btn-light btn-zoom" href="https://anhcodes.dev/contact">Get in
							touch</a>
					</div>
				</div>
			</div>
		</div>
		
			
			
		<div class="row footer__footer">
			<div class="col-lg-6">
				<div class="footer__footer_copy text-light">
					<p>Â©2023 - Made with â˜•, Hugo, and Firebase by <a class="text-light" href="https://anhcodes.dev/" target="_blank">Anh Chu</a></p>
				</div>
			</div>
			<div class="col-lg-6">
				<div class="footer__footer_social">
					<ul class="unstyle-list">
						
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://www.instagram.com/jasminmay12/"><i
									class="fa fa-instagram"></i></a>
						</li>
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://www.linkedin.com/in/anhhchu/"><i
									class="fa fa-linkedin-square"></i></a>
						</li>
						
						<li class="d-inline-block mx-2"><a class="text-light" target="_blank" href="https://github.com/anhhchu"><i
									class="fa fa-github-square"></i></a>
						</li>
						
					</ul>
				</div>
			</div>
		</div>
	</div>
</section>
<script src="https://maps.googleapis.com/maps/api/js?key=AIzaSyBRwXsfdEDBGKI-JcVzIfRyKNF2QBWWa-g&libraries=geometry"></script>
<script src="https://anhcodes.dev/plugins/jQuery/jquery.min.js"></script>
<script src="https://anhcodes.dev/plugins/bootstrap/bootstrap.min.js"></script>
<script src="https://anhcodes.dev/plugins/slick/slick.min.js"></script>
<script src="https://anhcodes.dev/plugins/waypoint/jquery.waypoints.min.js"></script>
<script src="https://anhcodes.dev/plugins/magnafic-popup/jquery.magnific-popup.min.js"></script>
<script src="https://anhcodes.dev/plugins/tweenmax/TweenMax.min.js"></script>
<script src="https://anhcodes.dev/plugins/imagesloaded/imagesloaded.min.js"></script>
<script src="https://anhcodes.dev/plugins/masonry/masonry.min.js"></script>


<script src="https://anhcodes.dev/js/script.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4.37.1"></script>
<script src="https://cdn.jsdelivr.net/npm/algoliasearch@4.11.0/dist/algoliasearch.umd.min.js"></script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css" integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin="anonymous">  

</body>

</html>