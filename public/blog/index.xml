<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data, Coding &amp; More on anhcodes</title>
    <link>https://anhcodes.dev/blog/</link>
    <description>Recent content in Data, Coding &amp; More on anhcodes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Apr 2023 23:20:57 +0000</lastBuildDate><atom:link href="https://anhcodes.dev/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to Scale and Tune Spark Effectively</title>
      <link>https://anhcodes.dev/blog/scale-spark/</link>
      <pubDate>Tue, 11 Apr 2023 23:20:57 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/scale-spark/</guid>
      <description>If you&amp;rsquo;re working with big data, you&amp;rsquo;ve probably heard of Spark. Spark is a distributed big data processing engine that provides in-memory storage for intermediate computations, making it much faster than Hadoop MapReduce. In this post, we&amp;rsquo;ll cover how to scale and tune Spark effectively to make the most of its capabilities.
1. What is Spark? Spark is a distributed big data processing engine that uses the resilient distributed dataset (RDD) data structure with data abstractions such as DataFrames and Datasets.</description>
    </item>
    
    <item>
      <title>Navigate Databricks FileSystem</title>
      <link>https://anhcodes.dev/blog/dbfs/</link>
      <pubDate>Tue, 11 Apr 2023 22:13:33 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/dbfs/</guid>
      <description> View Fullscreen ðŸ”— View on Github </description>
    </item>
    
    <item>
      <title>The beauty of a data lakehouse</title>
      <link>https://anhcodes.dev/blog/data-lakehouse/</link>
      <pubDate>Tue, 11 Apr 2023 16:26:45 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/data-lakehouse/</guid>
      <description>Choosing Enterprise Data Solution is a key challenge for modern businesses. With the explosion of data in recent years, organizations need to be able to store, manage, and analyze vast amounts of information to gain insights and make informed decisions. Data lakes, data warehouses, and data lakehouses are three popular solutions for managing data. However, Data Lakehouse has recently become a trend in distributed data management system
The battle: Data Warehouse vs Data Lake vs Data Lakehouse A data warehouse is a centralized repository that stores structured, processed data from a variety of sources.</description>
    </item>
    
    <item>
      <title>Deploy Debezium and Kafka on AKS using Strimzi Operator</title>
      <link>https://anhcodes.dev/blog/deploy-debezium-aks/</link>
      <pubDate>Sat, 07 Jan 2023 00:19:27 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/deploy-debezium-aks/</guid>
      <description>This tutorial follows Debezium official documentation Deploying Debezium on Kubernetes, but modified for Azure Kubernetes Service and Azure Container Registry.
Table of Content 1. Prerequisites
2. Deploy strimzi operator
3. Create secrets and roles
4. Deploy Kafka cluster
5. Deploy mysql database in Kubernetes
6. Deploy Kafka Connect Cluster
Step 1: Create secret for container registry Step 2: Build and push Kafka connecti image with debezium connector plugins to container registry Step 3: Deploy kafka connect cluster Step 4: Validate kafka connect cluster 7.</description>
    </item>
    
    <item>
      <title>How to recover lost files after a git reset --hard</title>
      <link>https://anhcodes.dev/blog/recover-lost-files-git-reset-hard/</link>
      <pubDate>Tue, 27 Dec 2022 11:30:20 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/recover-lost-files-git-reset-hard/</guid>
      <description>While working on a recent project, I accidentally committed some files. Instead of using git reset --soft &amp;lt;prev-commit-id&amp;gt; to unstage them, I used git reset --hard HEAD and all of my new changes gone with the wind. After panicking for a few minutes, I determined to learn how git reset works and how I can revert the damages.
1. What is git reset (hard vs soft) git reset --hard resets the current branch tip, and also deletes any changes in the working directory and staging area (although files under git stash will not be affected).</description>
    </item>
    
    <item>
      <title>Query Performance Tuning in Dedicated SQL Pool (Azure Synapse Analytics)</title>
      <link>https://anhcodes.dev/blog/query-performance/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/query-performance/</guid>
      <description>1. Best practices when Creating Tables When creating table in Dedicated SQL Pool, choose the correct Distribution Column and Index for best query performance. Follow Best Practices in creating tables in Azure Synapse Analytics
Remember to create and Update Stats on your new tables Table statistics for dedicated SQL pool in Azure Synapse Analytics
2. Identify slow-running QID and analyze compilation To best determine what is causing any given query&amp;rsquo;s slow performance, you will need to identify an example long-running query:</description>
    </item>
    
    <item>
      <title>Best Practices in creating tables in Azure Synapse Analytics</title>
      <link>https://anhcodes.dev/blog/create-table-azure-synapse-analytics/</link>
      <pubDate>Tue, 15 Nov 2022 23:24:30 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/create-table-azure-synapse-analytics/</guid>
      <description>As Dedicated SQL pool uses a scaled-out node based architecture, when creating tables in Dedicated SQL Pool, remember to specify the distribution (sharding strategy) and index for your tables.
Dedicated SQL Pool Architecture Choosing Distribution: The data is sharded into distributions to optimize the performance of the system. You can choose which sharding pattern to use to distribute the data when you define the table. These sharding patterns are supported:</description>
    </item>
    
  </channel>
</rss>
