<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>data-lakehouse on anhcodes</title>
    <link>https://anhcodes.dev/categories/data-lakehouse/</link>
    <description>Recent content in data-lakehouse on anhcodes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Jan 2024 17:01:45 +0000</lastBuildDate><atom:link href="https://anhcodes.dev/categories/data-lakehouse/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Simple way to write Spark Dataframe from Azure Databricks to Azure Synapse</title>
      <link>https://anhcodes.dev/blog/databricks-to-synapse/</link>
      <pubDate>Tue, 02 Jan 2024 17:01:45 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/databricks-to-synapse/</guid>
      <description>Reprequisites To follow along with this tutorial, you will need:
Azure subscription Synapse workspace Dedicated SQL Pool Azure Databricks workspace in the same subscription Step 1: Generate Spark dataframe in Databricks notebook Below spark code generates a dataframe of 3 columns and 100M rows (around 2GB of data)
import random from pyspark.sql.functions import rand, randn, col, lit from pyspark.sql.types import StructType, StructField, TimestampType, StringType, DecimalType, IntegerType schema = StructType([ StructField(&amp;#34;purchase_ts&amp;#34;, TimestampType(), True), StructField(&amp;#34;customer_id&amp;#34;, IntegerType(), True), StructField(&amp;#34;purchase_amount&amp;#34;, DecimalType(18,2), True) ]) df = spark.</description>
    </item>
    
    <item>
      <title>Data Objects in Databricks</title>
      <link>https://anhcodes.dev/blog/data-objects/</link>
      <pubDate>Sat, 30 Dec 2023 16:02:25 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/data-objects/</guid>
      <description>Metastore and Catalog In Databricks, metadata of data Objects (tables, views, etc.) is registered in Metastore.
Previously, Databricks uses Hive metastore by default to register schemas, tables, and views. However, it&amp;rsquo;s highly recommended to upgrade to Unity Catalog to access centralized access control, auditing, lineage, and data discovery capabilities.
Catalog is a group of databases. If you want to use Unity Catalog, create and specify Catalog is required (Set up and manage Unity Catalog).</description>
    </item>
    
    <item>
      <title>Deep Dive Into Delta Lake</title>
      <link>https://anhcodes.dev/blog/deep-dive-delta-lake/</link>
      <pubDate>Mon, 07 Aug 2023 11:23:00 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/deep-dive-delta-lake/</guid>
      <description>Delta Lake is an open source storage layer that brings reliability to data lakes. Delta Lake provides ACID transactions, scalable metadata handling, and unifies streaming and batch data processing. Delta Lake runs on top of your existing data lake and is fully compatible with Apache Spark APIs. Delta Lake uses versioned Parquet files to store your data in your cloud storage. Apart from the versions, Delta Lake also stores a transaction log to keep track of all the commits made to the table or blob store directory to provide ACID transactions.</description>
    </item>
    
    <item>
      <title>Navigate Databricks FileSystem</title>
      <link>https://anhcodes.dev/blog/dbfs/</link>
      <pubDate>Tue, 11 Apr 2023 22:13:33 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/dbfs/</guid>
      <description> View Fullscreen View on Github </description>
    </item>
    
    <item>
      <title>Query Performance Tuning in Dedicated SQL Pool (Azure Synapse Analytics)</title>
      <link>https://anhcodes.dev/blog/query-performance/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/query-performance/</guid>
      <description>1. Best practices when Creating Tables When creating table in Dedicated SQL Pool, choose the correct Distribution Column and Index for best query performance. Follow Best Practices in creating tables in Azure Synapse Analytics
Remember to create and Update Stats on your new tables Table statistics for dedicated SQL pool in Azure Synapse Analytics
2. Identify slow-running QID and analyze compilation To best determine what is causing any given query&amp;rsquo;s slow performance, you will need to identify an example long-running query:</description>
    </item>
    
    <item>
      <title>Azure Synapse Analytics - Data Lakehouse on Azure</title>
      <link>https://anhcodes.dev/blog/synapse/</link>
      <pubDate>Tue, 15 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://anhcodes.dev/blog/synapse/</guid>
      <description>What is Azure Synapse Analytics? Azure Synapse Analytics is an integrated data platform that brings together SQL technologies used in enterprise data warehousing such as Dedicated SQL Pool and Serverless SQL Pool, Synapse Spark Pool used for big data, Data Explorer for log and time series analytics, Synapse Pipelines for data integration and ETL/ELT, and deep integration with other Azure services such as Power BI, CosmosDB, and AzureML.
What is Dedicated SQL Pool?</description>
    </item>
    
  </channel>
</rss>
