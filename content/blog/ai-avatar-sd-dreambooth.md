---
title: "Create your own AI avatar with DreamBooth and Stable Diffusion"
date: 2023-05-05 18:06:47
featureImage: images/single-blog/diy-avatar/sd-dreambooth.png
# images/allpost/sql-pool-vertical.jpeg
postImage: images/single-blog/diy-avatar/grid-cyberfunk.png
caption: full head portrait of anh, cyberfunk, colorful trending on artstation, greg rutkowski, 4k -- By Author in Stable Diffusion
categories: ai
tags: [ai stable-diffusion]
author: Anh Chu
draft: True
---

## What is Stable Diffusion?

Stable Diffusion is a deep learning model that can generate detailed images based on text descriptions. Developed in collaboration with academic researchers and non-profit organizations by Stability AI, it is a kind of deep generative neural network known as a latent diffusion model. It can be applied to various tasks such as inpainting, outpainting, and generating image-to-image translations guided by a text prompt. Notably, Stable Diffusion's code and model weights have been publicly released and it can run on most consumer hardware equipped with a modest GPU with at least 8 GB VRAM. This makes it accessible to a wider range of users who can utilize the model to generate photo-realistic images based on text descriptions.

## What is DreamBooth?

DreamBooth is a deep learning generation model developed by researchers from Google Research and Boston University in 2022. It is used to fine-tune existing text-to-image models in order to produce personalized outputs based on several images of a particular subject. Originally developed using Google's own Imagen text-to-image model, DreamBooth implementations can be applied to other text-to-image models, where it allows for increased specificity when generating images of lesser-known subjects or rendering familiar subjects in different contexts. Users can customize their own text prompts and training data to produce more fine-tuned and personalized outputs.

## Options for Creating Your Own Images

There are several options available for creating your own AI avatars. Some of the paid options that I'm aware of include Lensa, RunwayML, and Leonardo AI. 

* Lensa is a digital portrait editor app availabe on Iphone and Android. You can upload your own photos on the mobile app, and it will transform your photos into customized avatars. The results are normally pretty good, but you have limited options to customize your images. And you have to pay extra for the new batch of images

* [RunwayML](https://runwayml.com/) is a creative platform that offers Generative AI images and videos editing tools. You can also upload your images and train your own custom AI model with Standard plan ($15/month at time of writing) with their [portrait generator service](https://app.runwayml.com/video-tools/teams/anhhchu12/ai-training/portrait-generator). It's worth noting that RunwayML was a partner of Stability AI, and the two companies have worked together on the development of Stable Diffusion, a text-to-image generative AI model. I haven't tried this yet so I'm not sure about the quality and consistency of the images generated by this tool 

* [Leonardo.Ai](https://leonardo.ai/) is a platform that utilizes artificial intelligence to generate various types of production-ready art assets.  In some senses, Leonardo.Ai is very similar to Midjourney, but you can generate images on WebUI as opposed to in Discord. Besides, Users can use a pre-trained or fine-tuned model to generate art assets, or generate prompts, or train their own AI model to generate custom assets. Currently, Midjourney doesn't provide the option to train your own model. As of the time of writing, Leonardo.Ai gives you 1 free model training credit that will reset every month, and 150 credits daily to generate AI images. I trained 1 model on this platform with about 8 images but the result is not as good as the DreamBooth free version. Below are some images created with Leonardo.Ai using my trained model. The images quality is definitely stunning, but they don't look like me, I don't even look Asian. Leonardo is definitely promising if I can get them to look like me, so I will try again when my credit resets

{{< image image="/images/single-blog/diy-avatar/leonardo.png" width=500  >}}

However, why pay if you can do it for free? From my experience, DreamBooth has been able to create pretty close rendering of my features from just a few 512x512 images without cost. So that's something I would like to show you in this post. 

## A 3-Step Process to Create Your Own AI Avatar for Free

### Step 1: Train Your Own Model with DreamBooth in Google Colabs

DreamBooth can be used in conjunction with Google Colabs to train your own stable diffusion models. 

1. Download and Import this [notebook](https://colab.research.google.com/github/sagiodev/stablediffusion_webui/blob/master/DreamBooth_Stable_Diffusion_SDA.ipynb). 

2. This notebook uses the `runwayml/stable-diffusion-v1-5` model on HuggingFace as the base model. You need to get the HuggingFace token on the website to continue. You can generate access tokens under `Settings/Access Tokens`

{{< image image="/images/single-blog/diy-avatar/hf-token.png" width=500  >}}

2. Change the instance_prompt to the subject name. For example, I use `anh` as the subject of my model (You will need to use this subject name in the prompt later). Keep the class_prompt as `photo of a person`

```python
    instance_prompt = "photo of anh person" #@param {type:"string"}
    class_prompt =  "photo of a person" #@param {type:"string"}
```

3. Upload 8 to 10 images of yourself, you should crop these images to 512x512 using the tool such as [birme.net](https://birme.net/) or canva. These models train well with images this size

4. Run the model training cell, it should complete in about 30-45 minutes. You should see a few rendering of the AI images after the training is done

5. Your model should be saved on Google Drive (Model saved to /content/drive/MyDrive/Dreambooth_model/model1.ckpt). You will need to use this model checkpoint in the next step. 

### Step 2: Download and Import the Model into Stable Diffusion WebUI

Stable Diffusion WebUI is a a user-friendly graphical interface created by AUTOMATIC111 that makes it easy to navigate, select models, and execute Stable Diffusion jobs quickly and efficiently. 

Follow this [GitHub repo](https://github.com/AUTOMATIC1111/stable-diffusion-webui) to install and launch this tool on your local machine.

Download the model in Google Drive (created in the previous step) and save under `stable-diffusion-webui/models/Stable-diffusion`

To spin up Stable Diffusion, `cd stable-diffusion-webui` and run `./webui.sh` in your terminal (on Mac). The webUI should run on your localhost `http://127.0.0.1:7860/`

Another option is to run this tool directly on Google Colabs (however, I'm not sure how much GPU you can use for free in Google Colabs). Follow this [guide](https://stable-diffusion-art.com/automatic1111-colab/) to set it up on Google Colab

### Step 3: Use text2image Option to Generate Your Images

Finally, choose the model checkpoint we just downloaded in the previous step. 

Then use the text2image option with the subject name to generate your images. For example, in below screenshot, `anh` is my subject name

{{< image image="/images/single-blog/diy-avatar/sdui.png" width=700  >}}


You can experiment with different settings and parameters to create truly unique and personalized avatars.


## Challenges and tips

AI images generated by Stable Diffusion are fascinating, but unpredictable and inconsistent. There is a high chance of distorted features in many of the images, so it's important to get a good prompt and some good configs. Below configs work pretty well for me

1. Use negative prompt to fix distorted features. Negative prompt is what you don't want to see in the images. It's a good rule of thumb to use negative prompt in open source SD model. Normally you don't have to this as much on Midjourney. I frequently use this prompt: `double heads, double bodies, ugly, tiling, poorly drawn hands, poorly drawn feet, poorly drawn face, out of frame, extra limbs, disfigured, deformed, body out of frame, bad anatomy, watermark, signature, cut off, low contrast, underexposed, overexposed, bad art, beginner, amateur, distorted face`

2. For sampling method, I frequently use `Euler a` and `DPM++ 2M Karras`. I'm not very familiar with different sampling methods, so it's more like trial and error. 

3. Generate about 4 images per batch so you have options to choose from

4. Keep everything else as default

5. If you are looking for a good prompt, use `lexica.art` to look for the styles and prompts you like. However, keep the prompt simple.

6. Use `Seed` to keep your images consistent. It works some of the time. 

{{< image image="/images/single-blog/diy-avatar/seed.png" width=700  >}}

7. SD WebUI provides several powerful options to perform image tasks such as inpainting, cropping, and upscaling. You can use the workflow below to edit your image further within the tool

{{< youtube vx2ZysNrUWw >}}

While there are a lot more you can do with SD WebUI, it's beyond the scope of this post. Just have fun experimenting with its various features, you sure can create impressive AI-generated assets. [stable-diffusion-art.com](https://stable-diffusion-art.com/) is a good resource to explore further.

